"""
æ•™è‚²å±€å®˜ç½‘ç»“æ„åŒ–é¢è¯•å…¬å‘Šçˆ¬è™«
ä¸“é—¨æŠ“å–å„åœ°æ•™è‚²å±€å‘å¸ƒçš„ç»“æ„åŒ–é¢è¯•å…¬å‘Š
"""

import os
import json
import hashlib
from typing import Dict, List
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from .base_scraper import BaseScraper


class GovSiteScraper(BaseScraper):
    """æ•™è‚²å±€å®˜ç½‘ç»“æ„åŒ–é¢è¯•å…¬å‘Šçˆ¬è™«"""

    # ç»“æ„åŒ–é¢è¯•å…³é”®è¯
    INTERVIEW_KEYWORDS = [
        "ç»“æ„åŒ–é¢è¯•",
        "é¢è¯•å®‰æ’",
        "é¢è¯•é€šçŸ¥",
        "é¢è¯•å…¬å‘Š",
        "ç­”è¾©",
        "é¢è¯•æ—¶é—´"
    ]

    def __init__(self, config: Dict):
        super().__init__(config)
        self.filters = config.get('filters', {})
        self.sites_config = config.get('data_sources', {}).get('gov_websites', {}).get('sites', {})

    def scrape(self, region: str = None, max_days: int = 90) -> List[Dict]:
        """
        æŠ“å–æŒ‡å®šåœ°åŒºçš„ç»“æ„åŒ–é¢è¯•å…¬å‘Š

        Args:
            region: åœ°åŒºåç§°ï¼ˆå¦‚"åŒ—äº¬"ï¼‰ï¼ŒNone è¡¨ç¤ºæŠ“å–æ‰€æœ‰åœ°åŒº
            max_days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å…¬å‘Š

        Returns:
            å…¬å‘Šåˆ—è¡¨
        """
        print(f"\nğŸ“ å¼€å§‹æŠ“å–æ•™è‚²å±€å®˜ç½‘å…¬å‘Š...")
        results = []

        # ç¡®å®šè¦æŠ“å–çš„åœ°åŒº
        regions = [region] if region else list(self.sites_config.keys())
        if not regions:
            print("  âš ï¸  æ²¡æœ‰é…ç½®åœ°åŒºç½‘ç«™")
            return results

        for region_name in regions:
            if region_name not in self.sites_config:
                print(f"  âš ï¸  è·³è¿‡æœªé…ç½®çš„åœ°åŒº: {region_name}")
                continue

            site_url = self.sites_config[region_name]
            print(f"\n  ğŸ“¡ æŠ“å– {region_name}: {site_url}")

            try:
                # å°è¯•æŠ“å–è¯¥åœ°åŒºçš„å…¬å‘Š
                announcements = self._fetch_announcements(region_name, site_url, max_days)
                results.extend(announcements)
                print(f"  âœ… {region_name} æŠ“å–åˆ° {len(announcements)} æ¡å…¬å‘Š")

            except Exception as e:
                print(f"  âŒ {region_name} æŠ“å–å¤±è´¥: {e}")
                continue

        print(f"\nğŸ“Š æ€»å…±æŠ“å–åˆ° {len(results)} æ¡å…¬å‘Š")
        return results

    def _fetch_announcements(self, region: str, site_url: str, max_days: int) -> List[Dict]:
        """
        æŠ“å–æŒ‡å®šç½‘ç«™çš„å…¬å‘Šåˆ—è¡¨

        Args:
            region: åœ°åŒºåç§°
            site_url: ç½‘ç«™é¦–é¡µ URL
            max_days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å…¬å‘Š

        Returns:
            å…¬å‘Šåˆ—è¡¨
        """
        announcements = []

        try:
            # æŠ“å–é¦–é¡µ
            response = self.fetch(site_url)
            if not response:
                return announcements

            soup = BeautifulSoup(response.text, 'lxml')

            # å°è¯•æŸ¥æ‰¾å…¬å‘Šåˆ—è¡¨
            # è¿™é‡Œéœ€è¦æ ¹æ®ä¸åŒç½‘ç«™çš„å®é™…æƒ…å†µè°ƒæ•´é€‰æ‹©å™¨
            # ç›®å‰ä½¿ç”¨é€šç”¨çš„ç­–ç•¥ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é’ˆå¯¹æ¯ä¸ªç½‘ç«™è¿›è¡Œé€‚é…

            # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«"å…¬å‘Š"ã€"é€šçŸ¥"ç­‰å…³é”®è¯çš„é“¾æ¥
            news_links = soup.find_all('a', href=True)

            cutoff_date = datetime.now() - timedelta(days=max_days)

            for link in news_links:
                try:
                    title = link.get_text(strip=True)
                    href = link['href']

                    # ç­›é€‰åŒ…å«é¢è¯•å…³é”®è¯çš„å…¬å‘Š
                    if not any(keyword in title for keyword in self.INTERVIEW_KEYWORDS):
                        continue

                    # æ„å»ºå®Œæ•´ URL
                    if href.startswith('/'):
                        base_url = '/'.join(site_url.split('/')[:3])
                        full_url = base_url + href
                    elif not href.startswith('http'):
                        full_url = site_url.rstrip('/') + '/' + href
                    else:
                        full_url = href

                    # æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆä½¿ç”¨ URL hashï¼‰
                    url_hash = hashlib.md5(full_url.encode()).hexdigest()

                    # åˆ›å»ºå…¬å‘Šè®°å½•
                    announcement = {
                        'region': region,
                        'title': title,
                        'url': full_url,
                        'url_hash': url_hash,
                        'found_at': datetime.now().isoformat()
                    }

                    announcements.append(announcement)

                    # é™åˆ¶æ•°é‡ï¼Œé¿å…æŠ“å–è¿‡å¤š
                    if len(announcements) >= 50:
                        break

                except Exception as e:
                    continue

        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")

        return announcements

    def _fetch_announcement_detail(self, url: str) -> str:
        """
        æŠ“å–å…¬å‘Šè¯¦æƒ…å†…å®¹

        Args:
            url: å…¬å‘Š URL

        Returns:
            å…¬å‘Šæ–‡æœ¬å†…å®¹
        """
        try:
            response = self.fetch(url)
            if not response:
                return ""

            soup = BeautifulSoup(response.text, 'lxml')

            # å°è¯•æå–æ­£æ–‡å†…å®¹
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(['script', 'style']):
                script.decompose()

            # è·å–æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)

            # æ¸…ç†ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)

            return cleaned_text[:10000]  # é™åˆ¶é•¿åº¦

        except Exception as e:
            print(f"  âŒ æŠ“å–è¯¦æƒ…å¤±è´¥: {e}")
            return ""
