"""
æ•™è‚²å±€å®˜ç½‘ç»“æ„åŒ–é¢è¯•å…¬å‘Šçˆ¬è™«
ä¸“é—¨æŠ“å–å„åœ°æ•™è‚²å±€å‘å¸ƒçš„ç»“æ„åŒ–é¢è¯•å…¬å‘Š
ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘æŠ“å– + ç¼“å­˜æœºåˆ¶
"""

import os
import json
import hashlib
from typing import Dict, List
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from .base_scraper import BaseScraper


class GovSiteScraper(BaseScraper):
    """æ•™è‚²å±€å®˜ç½‘ç»“æ„åŒ–é¢è¯•å…¬å‘Šçˆ¬è™«"""

    # ç»“æ„åŒ–é¢è¯•å…³é”®è¯
    INTERVIEW_KEYWORDS = [
        "ç»“æ„åŒ–é¢è¯•",
        "é¢è¯•å®‰æ’",
        "é¢è¯•é€šçŸ¥",
        "é¢è¯•å…¬å‘Š",
        "ç­”è¾©",
        "é¢è¯•æ—¶é—´"
    ]

    # æ•™å¸ˆæ‹›è˜å…³é”®è¯ï¼ˆæ”¾å®½èŒƒå›´ï¼Œä¼˜å…ˆçº§æ›´é«˜ï¼‰
    RECRUITMENT_KEYWORDS = [
        "æ•™å¸ˆæ‹›è˜",
        "æ‹›è˜æ•™å¸ˆ",
        "æ•™å¸ˆå½•ç”¨",
        "æ•™å¸ˆå¼•è¿›",
        "å…¬å¼€æ‹›è˜",
        "æ•™è‚²ç³»ç»Ÿ",
        "äº‹ä¸šå•ä½"
    ]

    def __init__(self, config: Dict):
        super().__init__(config)
        self.filters = config.get('filters', {})
        self.sites_config = config.get('data_sources', {}).get('gov_websites', {}).get('sites', {})
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'data', 'scraping_cache.json')
        self.cache = self._load_cache()

    def scrape(self, region: str = None, max_days: int = 90, max_workers: int = 5) -> List[Dict]:
        """
        æŠ“å–æŒ‡å®šåœ°åŒºçš„ç»“æ„åŒ–é¢è¯•å…¬å‘Šï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            region: åœ°åŒºåç§°ï¼ˆå¦‚"åŒ—äº¬"ï¼‰ï¼ŒNone è¡¨ç¤ºæŠ“å–æ‰€æœ‰åœ°åŒº
            max_days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å…¬å‘Š
            max_workers: å¹¶å‘çº¿ç¨‹æ•°

        Returns:
            å…¬å‘Šåˆ—è¡¨
        """
        print(f"\nğŸ“ å¼€å§‹æŠ“å–æ•™è‚²å±€å®˜ç½‘å…¬å‘Šï¼ˆå¹¶å‘æ¨¡å¼ï¼Œ{max_workers} çº¿ç¨‹ï¼‰...")
        results = []

        # ç¡®å®šè¦æŠ“å–çš„åœ°åŒº
        regions = [region] if region else list(self.sites_config.keys())
        if not regions:
            print("  âš ï¸  æ²¡æœ‰é…ç½®åœ°åŒºç½‘ç«™")
            return results

        print(f"  ğŸ“‹ è®¡åˆ’æŠ“å– {len(regions)} ä¸ªåœ°åŒº")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æŠ“å–
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_region = {}
            for region_name in regions:
                if region_name not in self.sites_config:
                    print(f"  âš ï¸  è·³è¿‡æœªé…ç½®çš„åœ°åŒº: {region_name}")
                    continue

                site_url = self.sites_config[region_name]
                print(f"  ğŸ“¡ æäº¤ä»»åŠ¡: {region_name}")

                future = executor.submit(self._fetch_announcements, region_name, site_url, max_days)
                future_to_region[future] = region_name

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_region):
                region_name = future_to_region[future]
                completed += 1

                try:
                    announcements = future.result()
                    results.extend(announcements)
                    print(f"  âœ… [{completed}/{len(future_to_region)}] {region_name}: {len(announcements)} æ¡")

                except Exception as e:
                    print(f"  âŒ [{completed}/{len(future_to_region)}] {region_name}: {str(e)[:50]}")
                    continue

        # æ›´æ–°ç¼“å­˜
        self._save_cache(results)

        print(f"\nğŸ“Š æ€»å…±æŠ“å–åˆ° {len(results)} æ¡å…¬å‘Š")
        return results

    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return {}

    def _save_cache(self, announcements: List[Dict]):
        """ä¿å­˜ç¼“å­˜"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            # åªä¿ç•™æœ€è¿‘1000æ¡
            cache_data = {
                'updated_at': datetime.now().isoformat(),
                'announcements': announcements[:1000]
            }
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _fetch_announcements(self, region: str, site_url: str, max_days: int) -> List[Dict]:
        """
        æŠ“å–æŒ‡å®šç½‘ç«™çš„å…¬å‘Šåˆ—è¡¨

        Args:
            region: åœ°åŒºåç§°
            site_url: ç½‘ç«™é¦–é¡µ URL
            max_days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©çš„å…¬å‘Š

        Returns:
            å…¬å‘Šåˆ—è¡¨
        """
        announcements = []

        try:
            # æŠ“å–é¦–é¡µ
            response = self.fetch(site_url)
            if not response:
                return announcements

            soup = BeautifulSoup(response.text, 'lxml')

            # å°è¯•æŸ¥æ‰¾å…¬å‘Šåˆ—è¡¨
            # è¿™é‡Œéœ€è¦æ ¹æ®ä¸åŒç½‘ç«™çš„å®é™…æƒ…å†µè°ƒæ•´é€‰æ‹©å™¨
            # ç›®å‰ä½¿ç”¨é€šç”¨çš„ç­–ç•¥ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é’ˆå¯¹æ¯ä¸ªç½‘ç«™è¿›è¡Œé€‚é…

            # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«"å…¬å‘Š"ã€"é€šçŸ¥"ç­‰å…³é”®è¯çš„é“¾æ¥
            news_links = soup.find_all('a', href=True)

            cutoff_date = datetime.now() - timedelta(days=max_days)

            for link in news_links:
                try:
                    title = link.get_text(strip=True)
                    href = link['href']

                    # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼ˆé¢è¯• + æ‹›è˜ï¼‰
                    all_keywords = self.INTERVIEW_KEYWORDS + self.RECRUITMENT_KEYWORDS

                    # ç­›é€‰åŒ…å«å…³é”®è¯çš„å…¬å‘Šï¼ˆæ”¾å®½èŒƒå›´ï¼‰
                    if not any(keyword in title for keyword in all_keywords):
                        continue

                    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜ï¼ˆå¯èƒ½æ˜¯å¯¼èˆªé“¾æ¥ï¼‰
                    if len(title) < 8:
                        continue

                    # æ„å»ºå®Œæ•´ URL
                    if href.startswith('/'):
                        base_url = '/'.join(site_url.split('/')[:3])
                        full_url = base_url + href
                    elif not href.startswith('http'):
                        full_url = site_url.rstrip('/') + '/' + href
                    else:
                        full_url = href

                    # æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆä½¿ç”¨ URL hashï¼‰
                    url_hash = hashlib.md5(full_url.encode()).hexdigest()

                    # åˆ›å»ºå…¬å‘Šè®°å½•
                    announcement = {
                        'region': region,
                        'title': title,
                        'url': full_url,
                        'url_hash': url_hash,
                        'found_at': datetime.now().isoformat()
                    }

                    announcements.append(announcement)

                    # é™åˆ¶æ•°é‡ï¼Œé¿å…æŠ“å–è¿‡å¤š
                    if len(announcements) >= 50:
                        break

                except Exception as e:
                    continue

        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")

        return announcements

    def _fetch_announcement_detail(self, url: str) -> str:
        """
        æŠ“å–å…¬å‘Šè¯¦æƒ…å†…å®¹

        Args:
            url: å…¬å‘Š URL

        Returns:
            å…¬å‘Šæ–‡æœ¬å†…å®¹
        """
        try:
            response = self.fetch(url)
            if not response:
                return ""

            soup = BeautifulSoup(response.text, 'lxml')

            # å°è¯•æå–æ­£æ–‡å†…å®¹
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(['script', 'style']):
                script.decompose()

            # è·å–æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)

            # æ¸…ç†ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)

            return cleaned_text[:10000]  # é™åˆ¶é•¿åº¦

        except Exception as e:
            print(f"  âŒ æŠ“å–è¯¦æƒ…å¤±è´¥: {e}")
            return ""
