"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«
é€šè¿‡æœç‹—å¾®ä¿¡æœç´¢æŠ“å–å…¬ä¼—å·æ–‡ç« 
"""

import hashlib
import requests
from typing import Dict, List
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from .base_scraper import BaseScraper


class WechatScraper(BaseScraper):
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«"""

    # æœç‹—å¾®ä¿¡æœç´¢ URL
    SOGOU_WEIXIN_SEARCH = "https://weixin.sogou.com/weixin"

    # æœç´¢å…³é”®è¯
    SEARCH_KEYWORDS = [
        "æ•™å¸ˆæ‹›è˜",
        "ç»“æ„åŒ–é¢è¯•",
        "æ•™å¸ˆç¼–åˆ¶",
        "é¢è¯•é€šçŸ¥",
        "æ‹›è˜å…¬å‘Š"
    ]

    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–å¾®ä¿¡çˆ¬è™«

        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.enabled = config.get('data_sources', {}).get('wechat', {}).get('enabled', False)
        self.max_results = config.get('data_sources', {}).get('wechat', {}).get('max_results', 20)

    def scrape(self, region: str = None, max_days: int = 90, max_workers: int = 1) -> List[Dict]:
        """
        é€šè¿‡æœç‹—å¾®ä¿¡æœç´¢æŠ“å–æ–‡ç« 

        Args:
            region: åœ°åŒºåç§°ï¼ˆå¯ç”¨äºé™å®šæœç´¢èŒƒå›´ï¼‰
            max_days: æœ€å¤§å¤©æ•°ï¼ˆæš‚ä¸ä½¿ç”¨ï¼‰
            max_workers: å¹¶å‘æ•°ï¼ˆæš‚ä¸ä½¿ç”¨ï¼‰

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        if not self.enabled:
            print("  âš ï¸  å¾®ä¿¡æ•°æ®æºæœªå¯ç”¨")
            return []

        print("\nğŸ“± ä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢")
        all_articles = []

        try:
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
            for keyword in self.SEARCH_KEYWORDS:
                articles = self._search_weixin(keyword, region)
                all_articles.extend(articles)

                # é™åˆ¶æ€»æ•°é‡
                if len(all_articles) >= self.max_results:
                    break

            print(f"  âœ… æ‰¾åˆ° {len(all_articles)} ç¯‡ç›¸å…³æ–‡ç« ")

        except Exception as e:
            print(f"  âŒ å¾®ä¿¡æœç´¢å¤±è´¥: {e}")

        return all_articles

    def _search_weixin(self, keyword: str, region: str = None) -> List[Dict]:
        """
        æœç‹—å¾®ä¿¡æœç´¢

        Args:
            keyword: æœç´¢å…³é”®è¯
            region: åœ°åŒºé™å®š

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = []

        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            if region:
                query = f"{region} {keyword}"
            else:
                query = keyword

            params = {
                'type': 2,  # 2 è¡¨ç¤ºæœç´¢æ–‡ç« 
                'query': query,
                'ie': 'utf8'
            }

            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }

            response = requests.get(
                self.SOGOU_WEIXIN_SEARCH,
                params=params,
                headers=headers,
                timeout=10
            )

            if response.status_code != 200:
                return articles

            soup = BeautifulSoup(response.text, 'html.parser')

            # æŸ¥æ‰¾æ–‡ç« ç»“æœ
            results = soup.find_all('div', class_='news-box')

            for item in results:
                try:
                    # æå–æ ‡é¢˜
                    title_elem = item.find('h3')
                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)

                    # æå–é“¾æ¥
                    link_elem = item.find('a')
                    if not link_elem:
                        continue

                    # æœç‹—çš„é“¾æ¥æ˜¯å¾®ä¿¡æ–‡ç« çš„è·³è½¬é“¾æ¥
                    sogou_url = link_elem.get('href', '')

                    # æå–å…¬ä¼—å·åç§°
                    account_elem = item.find('a', class_='account')
                    account = account_elem.get_text(strip=True) if account_elem else "æœªçŸ¥å…¬ä¼—å·"

                    # æå–æ‘˜è¦
                    summary_elem = item.find('p', class_='txt-info')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ""

                    # æå–æ—¶é—´
                    time_elem = item.find('span', class_='s2')
                    publish_time = time_elem.get_text(strip=True) if time_elem else ""

                    # ç”Ÿæˆå”¯ä¸€ ID
                    url_hash = hashlib.md5(sogou_url.encode()).hexdigest()

                    article = {
                        'region': region or 'å…¨å›½',
                        'title': title,
                        'url': sogou_url,  # ä½¿ç”¨æœç‹—é“¾æ¥
                        'url_hash': url_hash,
                        'account': account,
                        'summary': summary[:200],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                        'publish_time': publish_time,
                        'found_at': datetime.now().isoformat(),
                        'source': 'wechat'
                    }

                    articles.append(article)

                except Exception as e:
                    continue

        except Exception as e:
            print(f"  âŒ æœç´¢å¤±è´¥: {e}")

        return articles
