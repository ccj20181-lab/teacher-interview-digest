"""
ç»“æ„åŒ–é¢è¯•è€ƒæƒ… AI åˆ†æå™¨
ä½¿ç”¨ Claude AI ç”Ÿæˆç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†æç®€æŠ¥
"""

import os
import json
import anthropic
from typing import Dict, List


class InterviewAnalyzer:
    """ç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†æå™¨"""

    # ä¸»åˆ†æ Prompt
    STRUCTURED_INTERVIEW_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™å¸ˆæ‹›è˜è€ƒè¯•åˆ†æä¸“å®¶ï¼Œä¸“æ³¨äº**ç»“æ„åŒ–é¢è¯•**è€ƒæƒ…åˆ†æã€‚è¯·æ ¹æ®ä»¥ä¸‹æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†ææŠ¥å‘Šã€‚

ä»Šå¤©æ—¥æœŸ: {today}

## æ”¶é›†åˆ°çš„åŸå§‹æ‹›è˜å’Œé¢è¯•ä¿¡æ¯:
{content}

---

## è¯·ç”Ÿæˆä»¥ä¸‹ç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†ææŠ¥å‘Š:

### 1. ğŸ¯ å³å°†åˆ°æ¥çš„ç»“æ„åŒ–é¢è¯•ï¼ˆæ—¶é—´å€’åºï¼‰
**[ç´§æ€¥] 7å¤©å†…é¢è¯•**
- **[åœ°åŒº] XXå­¦æ ¡/æ•™è‚²å±€**
  - é¢è¯•æ—¶é—´: YYYY-MM-DD HH:MM
  - é¢è¯•åœ°ç‚¹: XXX
  - æŠ¥åæˆªæ­¢: YYYY-MM-DD
  - **å¿«é€Ÿå‡†å¤‡å»ºè®®**: 2-3æ¡ç´§æ€¥å¤‡è€ƒå»ºè®®
  - **å…¬å‘Šé“¾æ¥**: åŸå§‹é“¾æ¥

**[è¿‘æœŸ] 30å¤©å†…é¢è¯•**
- åˆ—å‡ºæ‰€æœ‰30å¤©å†…çš„ç»“æ„åŒ–é¢è¯•å®‰æ’
- æ ¼å¼åŒä¸Šï¼Œçªå‡ºå…³é”®æ—¶é—´èŠ‚ç‚¹

### 2. ğŸ“Š è¿‘æœŸç»“æ„åŒ–é¢è¯•è€ƒæƒ…æ±‡æ€»
- **é¢è¯•åœ°åŒºåˆ†å¸ƒ**: ç»Ÿè®¡å„åœ°åŒºé¢è¯•æ•°é‡
- **é¢è¯•æ—¶é—´é›†ä¸­æœŸ**: åˆ†æé¢è¯•é«˜å³°æœŸï¼ˆå¦‚5æœˆã€6æœˆï¼‰
- **é¢è¯•å½¢å¼è¶‹åŠ¿**: çº¯ç»“æ„åŒ– / ç»“æ„åŒ–+è¯•è®² / ç»“æ„åŒ–+è¯´è¯¾çš„æ¯”ä¾‹
- **çƒ­é—¨é¢˜å‹**: ç»Ÿè®¡é«˜é¢‘é¢˜å‹ï¼ˆç»¼åˆåˆ†æã€åº”æ€¥åº”å˜ã€äººé™…æ²Ÿé€šç­‰ï¼‰

### 3. ğŸ’ ç»“æ„åŒ–é¢è¯•çœŸé¢˜ç²¾é€‰ï¼ˆ5-8é“ï¼‰
ä»æ”¶é›†åˆ°çš„çœŸé¢˜ä¸­ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„é¢˜ç›®ï¼š
- **[åœ°åŒº] é¢˜ç›®ç±»å‹**: å…·ä½“é¢˜ç›®
  - **ç­”é¢˜æ€è·¯**: 200å­—å·¦å³çš„ç­”é¢˜æ¡†æ¶
  - **å‚è€ƒè¦ç‚¹**: 3-4ä¸ªå…³é”®å¾—åˆ†ç‚¹
  - **æ¥æº**: XXåœ°åŒº 202Xå¹´é¢è¯•çœŸé¢˜

### 4. ğŸ“ˆ è€ƒæƒ…è¶‹åŠ¿åˆ†æ
- **é¢è¯•éš¾åº¦å˜åŒ–**: ä¸å¾€å¹´ç›¸æ¯”çš„éš¾åº¦æå‡æˆ–é™ä½
- **é¢˜å‹æ–°è¶‹åŠ¿**: æ˜¯å¦å‡ºç°æ–°çš„é¢˜å‹æˆ–è€ƒå¯Ÿæ–¹å‘
- **åœ°åŒºç‰¹è‰²**: ä¸åŒåœ°åŒºçš„é¢è¯•ç‰¹ç‚¹ï¼ˆå¦‚æŸäº›åœ°åŒºåé‡æ•™è‚²çƒ­ç‚¹ï¼‰
- **ç«äº‰æ¿€çƒˆåº¦**: åŸºäºæ‹›è˜äººæ•°å’ŒæŠ¥åæƒ…å†µçš„ç«äº‰åˆ†æ

### 5. ğŸ“ é«˜é¢‘è€ƒç‚¹é€ŸæŸ¥
| è€ƒç‚¹ç±»åˆ« | é«˜é¢‘é¢˜ç›®ä¸¾ä¾‹ | å‡ºç°é¢‘ç‡ | åœ°åŒº |
|---------|-------------|----------|------|
| ç»¼åˆåˆ†æç±» | "å¦‚ä½•çœ‹å¾…åŒå‡æ”¿ç­–ï¼Ÿ" | é«˜ | å…¨å›½ |
| åº”æ€¥åº”å˜ç±» | "å­¦ç”Ÿè¯¾å ‚å†²çªå¦‚ä½•å¤„ç†ï¼Ÿ" | é«˜ | å…¨å›½ |
| ... | ... | ... | ... |

### 6. ğŸ’¡ å¤‡è€ƒç­–ç•¥å»ºè®®
#### æŒ‰é¢è¯•æ—¶é—´å€’æ¨çš„å¤‡è€ƒè®¡åˆ’
- **é¢è¯•å‰7å¤©**: é‡ç‚¹çªç ´ã€æ¨¡æ‹Ÿç»ƒä¹ 
- **é¢è¯•å‰1ä¸ªæœˆ**: ç³»ç»Ÿå¤ä¹ ã€é¢˜åº“ç§¯ç´¯
- **é¢è¯•å‰3ä¸ªæœˆ**: åŸºç¡€å­¦ä¹ ã€æ¡†æ¶æ­å»º

#### é’ˆå¯¹ä¸åŒé¢˜å‹çš„å¤‡è€ƒæŠ€å·§
- ç»¼åˆåˆ†æç±»: **æ˜¯ä»€ä¹ˆ-ä¸ºä»€ä¹ˆ-æ€ä¹ˆåš-å‡å**
- åº”æ€¥åº”å˜ç±»: **è½»é‡ç¼“æ€¥-å¤šæ–¹åè°ƒ-æ€»ç»“åæ€**
- äººé™…æ²Ÿé€šç±»: **æ€åº¦å°Šé‡-æœ‰æ•ˆæ²Ÿé€š-è§£å†³çŸ›ç›¾**

### 7. ğŸ”— é‡è¦èµ„æºé“¾æ¥
- **é¢è¯•å…¬å‘Šæ±‡æ€»**: æœ€æ–°å…¬å‘Šé“¾æ¥åˆ—è¡¨
- **çœŸé¢˜èµ„æº**: å†å¹´çœŸé¢˜æ±‡æ€»é“¾æ¥
- **å¤‡è€ƒèµ„æ–™**: æ¨èçš„æ•™æå’Œé¢˜åº“
- **å­¦ä¹ ç¤¾ç¾¤**: ç›¸å…³çš„å¤‡è€ƒç¾¤æˆ–è®ºå›

### 8. â° ä¸‹ä¸€æ­¥è¡ŒåŠ¨æé†’
ä¸ºè€ƒç”Ÿæä¾›æ˜ç¡®çš„æ—¶é—´çº¿ï¼š
- è¿‘æœŸæŠ¥åæˆªæ­¢ï¼ˆ3å¤©å†…ï¼‰
- è¿‘æœŸé¢è¯•æé†’ï¼ˆ7å¤©å†…ã€30å¤©å†…ï¼‰
- é•¿æœŸå¤‡è€ƒå»ºè®®ï¼ˆ3ä¸ªæœˆä»¥ä¸Šï¼‰

---

## è¾“å‡ºæ ¼å¼è¦æ±‚:
- ä½¿ç”¨ Markdown æ ¼å¼
- é‡ç‚¹çªå‡º**é¢è¯•æ—¶é—´**ä¿¡æ¯ï¼ˆä½¿ç”¨åŠ ç²—å’Œè¡¨æƒ…ï¼‰
- ä¿ç•™æ‰€æœ‰åŸå§‹é“¾æ¥
- æ€»é•¿åº¦æ§åˆ¶åœ¨ 2500 å­—ä»¥å†…
- ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æ„åŒ–å…ƒç´ æå‡å¯è¯»æ€§
- **å¿…é¡»çªå‡ºæ˜¾ç¤º7å¤©å†…å³å°†åˆ°æ¥çš„é¢è¯•**

è¯·ç›´æ¥è¾“å‡ºåˆ†ææŠ¥å‘Šï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜ã€‚"""

    # ç»“æ„åŒ–ä¿¡æ¯æå– Prompt
    EXTRACT_INTERVIEW_INFO_PROMPT = """ä»ä»¥ä¸‹æ•™å¸ˆæ‹›è˜å…¬å‘Šä¸­æå–**ç»“æ„åŒ–é¢è¯•**ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š

å…¬å‘Šæ–‡æœ¬:
{text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆä»¥ JSON æ ¼å¼è¿”å›ï¼‰:
{{
    "region": "åœ°åŒºåç§°ï¼ˆçœ/å¸‚ï¼‰",
    "organization": "æ‹›è˜å•ä½åç§°",
    "announcement_title": "å…¬å‘Šæ ‡é¢˜",
    "announcement_url": "å…¬å‘Šé“¾æ¥",
    "recruitment_count": æ‹›è˜äººæ•°ï¼ˆæ•°å­—ï¼‰,
    "registration_period": {{
        "start": "æŠ¥åå¼€å§‹æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰",
        "end": "æŠ¥åæˆªæ­¢æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰"
    }},
    "written_exam_date": "ç¬”è¯•æ—¶é—´ï¼ˆYYYY-MM-DDï¼Œå¦‚æ— åˆ™nullï¼‰",
    "structured_interview": {{
        "has_interview": true/falseï¼ˆæ˜¯å¦åŒ…å«ç»“æ„åŒ–é¢è¯•ï¼‰,
        "interview_date": "é¢è¯•æ—¥æœŸï¼ˆYYYY-MM-DDï¼Œå¦‚æœªç¡®å®šåˆ™nullï¼‰",
        "interview_time": "é¢è¯•å…·ä½“æ—¶é—´ï¼ˆå¦‚æœ‰ï¼‰",
        "interview_location": "é¢è¯•åœ°ç‚¹",
        "interview_format": "é¢è¯•å½¢å¼ï¼ˆå¦‚ï¼šçº¯ç»“æ„åŒ–ã€ç»“æ„åŒ–+è¯•è®²ã€ç»“æ„åŒ–+è¯´è¯¾ï¼‰",
        "question_types": ["é¢˜å‹1", "é¢˜å‹2"],
        "interview_duration": "é¢è¯•æ—¶é•¿ï¼ˆå¦‚ï¼š15åˆ†é’Ÿï¼‰",
        "preparation_time": "å¤‡è€ƒæ—¶é—´ï¼ˆå¦‚ï¼š5åˆ†é’Ÿï¼‰"
    }},
    "special_requirements": "ç‰¹æ®Šè¦æ±‚æˆ–å¤‡æ³¨",
    "publish_date": "å…¬å‘Šå‘å¸ƒæ—¶é—´ï¼ˆYYYY-MM-DDï¼‰"
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœå…¬å‘Šä¸­æ²¡æœ‰æ˜ç¡®æåˆ°"ç»“æ„åŒ–é¢è¯•"ï¼Œåˆ™ has_interview è®¾ä¸º false
2. åªæå–æ˜ç¡®çš„ä¿¡æ¯ï¼Œä¸ç¡®å®šçš„å­—æ®µè®¾ä¸º null
3. é¢è¯•å½¢å¼éœ€è¦æ ¹æ®å…¬å‘Šæè¿°å‡†ç¡®åˆ¤æ–­ï¼ˆå¦‚"ç­”è¾©"ã€"é—®ç­”"é€šå¸¸æŒ‡ç»“æ„åŒ–é¢è¯•ï¼‰
4. æ—¶é—´æ ¼å¼ç»Ÿä¸€ä¸º YYYY-MM-DD
5. å¿…é¡»è¿”å›çº¯ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—è¯´æ˜
"""

    def __init__(self, api_key: str, base_url: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            api_key: Anthropic API å¯†é’¥
            base_url: API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
        """
        self.client = anthropic.Anthropic(api_key=api_key, base_url=base_url)
        self.ai_config = {
            "model": "glm-4-plus",
            "max_tokens": 8192,
            "temperature": 0.3
        }

    def generate_interview_digest(
        self,
        announcements: List[Dict],
        questions: List[Dict],
        today: str
    ) -> str:
        """
        ç”Ÿæˆç»“æ„åŒ–é¢è¯•è€ƒæƒ…ç®€æŠ¥

        Args:
            announcements: å…¬å‘Šåˆ—è¡¨
            questions: çœŸé¢˜åˆ—è¡¨
            today: ä»Šå¤©çš„æ—¥æœŸ

        Returns:
            ç”Ÿæˆçš„ç®€æŠ¥å†…å®¹
        """
        # å‡†å¤‡å†…å®¹
        content = self._prepare_content(announcements, questions)

        print(f"\nğŸ¤– è°ƒç”¨ Claude API ç”Ÿæˆç®€æŠ¥...")
        print(f"  - å…¬å‘Šæ•°é‡: {len(announcements)}")
        print(f"  - çœŸé¢˜æ•°é‡: {len(questions)}")

        try:
            # è°ƒç”¨ Claude API
            response = self.client.messages.create(
                model=self.ai_config["model"],
                max_tokens=self.ai_config["max_tokens"],
                temperature=self.ai_config["temperature"],
                messages=[{
                    "role": "user",
                    "content": self.STRUCTURED_INTERVIEW_ANALYSIS_PROMPT.format(
                        today=today,
                        content=content
                    )
                }]
            )

            digest = response.content[0].text
            print(f"âœ… ç®€æŠ¥ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(digest)} å­—ç¬¦")
            return digest

        except Exception as e:
            print(f"âŒ ç”Ÿæˆç®€æŠ¥å¤±è´¥: {e}")
            return self._generate_fallback_digest(announcements, today)

    def extract_interview_info(self, announcement_text: str, url: str) -> Dict:
        """
        ä»å…¬å‘Šä¸­æå–ç»“æ„åŒ–é¢è¯•ä¿¡æ¯

        Args:
            announcement_text: å…¬å‘Šæ–‡æœ¬
            url: å…¬å‘Š URL

        Returns:
            æå–çš„ç»“æ„åŒ–ä¿¡æ¯
        """
        try:
            prompt = self.EXTRACT_INTERVIEW_INFO_PROMPT.format(
                text=announcement_text[:5000]  # é™åˆ¶é•¿åº¦
            )

            response = self.client.messages.create(
                model=self.ai_config["model"],
                max_tokens=2048,
                temperature=0.2,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # è§£æ JSON ç»“æœ
            result_text = response.content[0].text
            result = json.loads(result_text)
            result['announcement_url'] = url

            return result

        except Exception as e:
            print(f"  âš ï¸  æå–ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "has_interview": False,
                "error": str(e)
            }

    def _prepare_content(self, announcements: List[Dict], questions: List[Dict]) -> str:
        """å‡†å¤‡å‘é€ç»™ AI çš„å†…å®¹"""
        content_parts = []

        # æ·»åŠ å…¬å‘Šä¿¡æ¯
        if announcements:
            content_parts.append("## æ‹›è˜å…¬å‘Šä¿¡æ¯\n")
            for i, ann in enumerate(announcements[:20], 1):  # é™åˆ¶æ•°é‡
                content_parts.append(f"{i}. **{ann.get('title', 'æœªçŸ¥æ ‡é¢˜')}**")
                content_parts.append(f"   - åœ°åŒº: {ann.get('region', 'æœªçŸ¥')}")
                content_parts.append(f"   - é“¾æ¥: {ann.get('url', 'æ— ')}")
                if 'found_at' in ann:
                    content_parts.append(f"   - å‘ç°æ—¶é—´: {ann['found_at']}")
                content_parts.append("")

        # æ·»åŠ çœŸé¢˜ä¿¡æ¯
        if questions:
            content_parts.append("\n## é¢è¯•çœŸé¢˜ä¿¡æ¯\n")
            for i, q in enumerate(questions[:10], 1):
                content_parts.append(f"{i}. {q.get('question', 'æœªçŸ¥é¢˜ç›®')}")

        return '\n'.join(content_parts)

    def _generate_fallback_digest(self, announcements: List[Dict], today: str) -> str:
        """ç”Ÿæˆç®€åŒ–çš„å¤‡ç”¨ç®€æŠ¥"""
        lines = [
            f"# æ•™å¸ˆè€ƒç¼–ç»“æ„åŒ–é¢è¯•è€ƒæƒ…ç®€æŠ¥ ({today})",
            "",
            "âš ï¸ **æ³¨æ„**: AI ç”Ÿæˆå¤±è´¥ï¼Œä»¥ä¸‹ä¸ºç®€åŒ–ç‰ˆæœ¬",
            "",
            "## ğŸ“Š ä»Šæ—¥æ•°æ®ç»Ÿè®¡",
            f"- æŠ“å–åˆ° {len(announcements)} æ¡ç›¸å…³å…¬å‘Š",
            "",
            "## ğŸ“‹ å…¬å‘Šåˆ—è¡¨",
            ""
        ]

        for i, ann in enumerate(announcements[:20], 1):
            lines.append(f"{i}. **{ann.get('title', 'æœªçŸ¥')}**")
            lines.append(f"   - åœ°åŒº: {ann.get('region', 'æœªçŸ¥')}")
            lines.append(f"   - é“¾æ¥: {ann.get('url', 'æ— ')}")
            lines.append("")

        return '\n'.join(lines)
