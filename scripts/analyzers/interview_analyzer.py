"""
ç»“æ„åŒ–é¢è¯•è€ƒæƒ… AI åˆ†æå™¨
ä½¿ç”¨ Claude AI ç”Ÿæˆç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†æç®€æŠ¥
"""

import os
import json
import anthropic
from typing import Dict, List
from datetime import datetime


class InterviewAnalyzer:
    """ç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†æå™¨"""

    # é™æ€ç®€æŠ¥æ¨¡æ¿ï¼ˆç©ºæ•°æ®æ—¶ä½¿ç”¨ï¼‰
    STATIC_DIGEST_TEMPLATE = """# æ•™å¸ˆè€ƒç¼–ç»“æ„åŒ–é¢è¯•è€ƒæƒ…ç®€æŠ¥ ({today})

> â° æ›´æ–°æ—¶é—´: {datetime}

---

## âš ï¸ ä»Šæ—¥æ•°æ®çŠ¶æ€

**å½“å‰çŠ¶æ€**: æœªæ”¶é›†åˆ°æ–°çš„ç»“æ„åŒ–é¢è¯•ç›¸å…³ä¿¡æ¯

**æ•°æ®ç»Ÿè®¡**:
- ğŸ” ä»Šæ—¥æŠ“å–å…¬å‘Š: {announcement_count} æ¡
- ğŸ“ æ”¶é›†åˆ°çœŸé¢˜: {question_count} é“
- âœ… æ•°æ®æŠ“å–æ­£å¸¸: {is_normal}

---

## ğŸ’¡ è¯´æ˜

ä»Šæ—¥æœªæ£€æµ‹åˆ°æ–°çš„ç»“æ„åŒ–é¢è¯•ç›¸å…³å…¬å‘Šã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š
1. å„åœ°æ•™è‚²å±€/å­¦æ ¡æš‚æœªå‘å¸ƒæ–°çš„æ‹›è˜å…¬å‘Š
2. å…¬å‘Šå‘å¸ƒæ—¶é—´ä¸åœ¨æˆ‘ä»¬çš„æŠ“å–å‘¨æœŸå†…
3. ç½‘ç«™ç»“æ„å˜åŒ–å¯¼è‡´æŠ“å–å¤±è´¥ï¼ˆæˆ‘ä»¬ä¼šåŠæ—¶ä¿®å¤ï¼‰

---

## ğŸ“ å¸¸å¤‡è€ƒå»ºè®®

è™½ç„¶æ²¡æœ‰æ–°çš„é¢è¯•ä¿¡æ¯ï¼Œä½†æ‚¨å¯ä»¥ï¼š

### æŒç»­å¤‡è€ƒ
1. **å…³æ³¨ç»“æ„åŒ–é¢è¯•é«˜é¢‘é¢˜å‹**:
   - ç»¼åˆåˆ†æç±»ï¼ˆæ•™è‚²æ”¿ç­–ã€ç°è±¡åˆ†æï¼‰
   - åº”æ€¥åº”å˜ç±»ï¼ˆè¯¾å ‚çªå‘äº‹ä»¶å¤„ç†ï¼‰
   - äººé™…æ²Ÿé€šç±»ï¼ˆå®¶æ ¡æ²Ÿé€šã€åŒäº‹åä½œï¼‰
   - èŒä¸šè®¤çŸ¥ç±»ï¼ˆæ•™å¸ˆèŒä¸šç†è§£ï¼‰

2. **ç§¯ç´¯æ•™è‚²çƒ­ç‚¹ç´ æ**:
   - åŒå‡æ”¿ç­–ã€äº”è‚²å¹¶ä¸¾
   - æ–°è¯¾æ ‡ã€æ ¸å¿ƒç´ å…»
   - åŠ³åŠ¨æ•™è‚²ã€å¿ƒç†å¥åº·

3. **å…³æ³¨å®˜æ–¹æ¸ é“**:
   - å„åœ°æ•™è‚²å±€å®˜ç½‘
   - äººç¤¾å±€æ‹›è˜å…¬å‘Š
   - æ•™è‚²ç±»æ‹›è˜ç½‘ç«™

---

## ğŸ”— èµ„æºæ¨è

- **å…¬å‘Šæ¥æº**: å…¨å›½å„åœ°æ•™è‚²å±€ã€äººç¤¾å±€å®˜ç½‘
- **æ›´æ–°é¢‘ç‡**: æ¯æ—¥è‡ªåŠ¨æŠ“å–
- **ä¸‹æ¬¡æ›´æ–°**: æ˜æ—¥ 7:00

---

*æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚æœ‰ç–‘é—®è¯·æŸ¥çœ‹åŸå§‹å…¬å‘Šé“¾æ¥*
"""

    # ä¸»åˆ†æ Prompt
    STRUCTURED_INTERVIEW_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™å¸ˆæ‹›è˜è€ƒè¯•åˆ†æä¸“å®¶ï¼Œä¸“æ³¨äº**ç»“æ„åŒ–é¢è¯•**è€ƒæƒ…åˆ†æã€‚

## âš ï¸ æ ¸å¿ƒåŸåˆ™ï¼ˆå¼ºåˆ¶æ‰§è¡Œï¼‰

1. **ç»å¯¹ç¦æ­¢ç¼–é€ ä¿¡æ¯**ï¼šå¦‚æœæ²¡æœ‰æ•°æ®ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜ï¼Œä¸å¾—è™šæ„ä»»ä½•å†…å®¹
2. **æ•°æ®çœŸå®æ€§**ï¼šåªåˆ†ææä¾›çš„åŸå§‹æ•°æ®ï¼Œä¸å¾—æ·»åŠ æœªç»è¯å®çš„ä¿¡æ¯
3. **ç©ºæ•°æ®å¤„ç†**ï¼šå½“æ•°æ®ä¸ºç©ºæ—¶ï¼Œæ˜ç¡®è¯´æ˜"ä»Šæ—¥æœªæ”¶é›†åˆ°æ–°ä¿¡æ¯"
4. **é“¾æ¥éªŒè¯**ï¼šåªä½¿ç”¨æä¾›çš„çœŸå®é“¾æ¥ï¼Œä¸å¾—ç¼–é€ é“¾æ¥

## å½“å‰æ•°æ®çŠ¶æ€

- å…¬å‘Šæ•°é‡ï¼š{announcement_count}
- çœŸé¢˜æ•°é‡ï¼š{question_count}
- æ•°æ®çŠ¶æ€ï¼š{data_status}

{empty_data_notice}

## æ”¶é›†åˆ°çš„åŸå§‹æ‹›è˜å’Œé¢è¯•ä¿¡æ¯:
{content}

---

## è¯·ç”Ÿæˆä»¥ä¸‹ç»“æ„åŒ–é¢è¯•è€ƒæƒ…åˆ†ææŠ¥å‘Š:

### 1. ğŸ¯ å³å°†åˆ°æ¥çš„ç»“æ„åŒ–é¢è¯•ï¼ˆæ—¶é—´å€’åºï¼‰
**[ç´§æ€¥] 7å¤©å†…é¢è¯•**
- **[åœ°åŒº] XXå­¦æ ¡/æ•™è‚²å±€**
  - é¢è¯•æ—¶é—´: YYYY-MM-DD HH:MM
  - é¢è¯•åœ°ç‚¹: XXX
  - æŠ¥åæˆªæ­¢: YYYY-MM-DD
  - **å¿«é€Ÿå‡†å¤‡å»ºè®®**: 2-3æ¡ç´§æ€¥å¤‡è€ƒå»ºè®®
  - **å…¬å‘Šé“¾æ¥**: åŸå§‹é“¾æ¥

**[è¿‘æœŸ] 30å¤©å†…é¢è¯•**
- åˆ—å‡ºæ‰€æœ‰30å¤©å†…çš„ç»“æ„åŒ–é¢è¯•å®‰æ’
- æ ¼å¼åŒä¸Šï¼Œçªå‡ºå…³é”®æ—¶é—´èŠ‚ç‚¹

### 2. ğŸ“Š è¿‘æœŸç»“æ„åŒ–é¢è¯•è€ƒæƒ…æ±‡æ€»
- **é¢è¯•åœ°åŒºåˆ†å¸ƒ**: ç»Ÿè®¡å„åœ°åŒºé¢è¯•æ•°é‡
- **é¢è¯•æ—¶é—´é›†ä¸­æœŸ**: åˆ†æé¢è¯•é«˜å³°æœŸï¼ˆå¦‚5æœˆã€6æœˆï¼‰
- **é¢è¯•å½¢å¼è¶‹åŠ¿**: çº¯ç»“æ„åŒ– / ç»“æ„åŒ–+è¯•è®² / ç»“æ„åŒ–+è¯´è¯¾çš„æ¯”ä¾‹
- **çƒ­é—¨é¢˜å‹**: ç»Ÿè®¡é«˜é¢‘é¢˜å‹ï¼ˆç»¼åˆåˆ†æã€åº”æ€¥åº”å˜ã€äººé™…æ²Ÿé€šç­‰ï¼‰

### 3. ğŸ’ ç»“æ„åŒ–é¢è¯•çœŸé¢˜ç²¾é€‰ï¼ˆ5-8é“ï¼‰
ä»æ”¶é›†åˆ°çš„çœŸé¢˜ä¸­ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„é¢˜ç›®ï¼š
- **[åœ°åŒº] é¢˜ç›®ç±»å‹**: å…·ä½“é¢˜ç›®
  - **ç­”é¢˜æ€è·¯**: 200å­—å·¦å³çš„ç­”é¢˜æ¡†æ¶
  - **å‚è€ƒè¦ç‚¹**: 3-4ä¸ªå…³é”®å¾—åˆ†ç‚¹
  - **æ¥æº**: XXåœ°åŒº 202Xå¹´é¢è¯•çœŸé¢˜

### 4. ğŸ“ˆ è€ƒæƒ…è¶‹åŠ¿åˆ†æ
- **é¢è¯•éš¾åº¦å˜åŒ–**: ä¸å¾€å¹´ç›¸æ¯”çš„éš¾åº¦æå‡æˆ–é™ä½
- **é¢˜å‹æ–°è¶‹åŠ¿**: æ˜¯å¦å‡ºç°æ–°çš„é¢˜å‹æˆ–è€ƒå¯Ÿæ–¹å‘
- **åœ°åŒºç‰¹è‰²**: ä¸åŒåœ°åŒºçš„é¢è¯•ç‰¹ç‚¹ï¼ˆå¦‚æŸäº›åœ°åŒºåé‡æ•™è‚²çƒ­ç‚¹ï¼‰
- **ç«äº‰æ¿€çƒˆåº¦**: åŸºäºæ‹›è˜äººæ•°å’ŒæŠ¥åæƒ…å†µçš„ç«äº‰åˆ†æ

### 5. ğŸ“ é«˜é¢‘è€ƒç‚¹é€ŸæŸ¥
| è€ƒç‚¹ç±»åˆ« | é«˜é¢‘é¢˜ç›®ä¸¾ä¾‹ | å‡ºç°é¢‘ç‡ | åœ°åŒº |
|---------|-------------|----------|------|
| ç»¼åˆåˆ†æç±» | "å¦‚ä½•çœ‹å¾…åŒå‡æ”¿ç­–ï¼Ÿ" | é«˜ | å…¨å›½ |
| åº”æ€¥åº”å˜ç±» | "å­¦ç”Ÿè¯¾å ‚å†²çªå¦‚ä½•å¤„ç†ï¼Ÿ" | é«˜ | å…¨å›½ |
| ... | ... | ... | ... |

### 6. ğŸ’¡ å¤‡è€ƒç­–ç•¥å»ºè®®
#### æŒ‰é¢è¯•æ—¶é—´å€’æ¨çš„å¤‡è€ƒè®¡åˆ’
- **é¢è¯•å‰7å¤©**: é‡ç‚¹çªç ´ã€æ¨¡æ‹Ÿç»ƒä¹ 
- **é¢è¯•å‰1ä¸ªæœˆ**: ç³»ç»Ÿå¤ä¹ ã€é¢˜åº“ç§¯ç´¯
- **é¢è¯•å‰3ä¸ªæœˆ**: åŸºç¡€å­¦ä¹ ã€æ¡†æ¶æ­å»º

#### é’ˆå¯¹ä¸åŒé¢˜å‹çš„å¤‡è€ƒæŠ€å·§
- ç»¼åˆåˆ†æç±»: **æ˜¯ä»€ä¹ˆ-ä¸ºä»€ä¹ˆ-æ€ä¹ˆåš-å‡å**
- åº”æ€¥åº”å˜ç±»: **è½»é‡ç¼“æ€¥-å¤šæ–¹åè°ƒ-æ€»ç»“åæ€**
- äººé™…æ²Ÿé€šç±»: **æ€åº¦å°Šé‡-æœ‰æ•ˆæ²Ÿé€š-è§£å†³çŸ›ç›¾**

### 7. ğŸ”— é‡è¦èµ„æºé“¾æ¥
- **é¢è¯•å…¬å‘Šæ±‡æ€»**: æœ€æ–°å…¬å‘Šé“¾æ¥åˆ—è¡¨
- **çœŸé¢˜èµ„æº**: å†å¹´çœŸé¢˜æ±‡æ€»é“¾æ¥
- **å¤‡è€ƒèµ„æ–™**: æ¨èçš„æ•™æå’Œé¢˜åº“
- **å­¦ä¹ ç¤¾ç¾¤**: ç›¸å…³çš„å¤‡è€ƒç¾¤æˆ–è®ºå›

### 8. â° ä¸‹ä¸€æ­¥è¡ŒåŠ¨æé†’
ä¸ºè€ƒç”Ÿæä¾›æ˜ç¡®çš„æ—¶é—´çº¿ï¼š
- è¿‘æœŸæŠ¥åæˆªæ­¢ï¼ˆ3å¤©å†…ï¼‰
- è¿‘æœŸé¢è¯•æé†’ï¼ˆ7å¤©å†…ã€30å¤©å†…ï¼‰
- é•¿æœŸå¤‡è€ƒå»ºè®®ï¼ˆ3ä¸ªæœˆä»¥ä¸Šï¼‰

---

## è¾“å‡ºæ ¼å¼è¦æ±‚:
- ä½¿ç”¨ Markdown æ ¼å¼
- é‡ç‚¹çªå‡º**é¢è¯•æ—¶é—´**ä¿¡æ¯ï¼ˆä½¿ç”¨åŠ ç²—å’Œè¡¨æƒ…ï¼‰
- ä¿ç•™æ‰€æœ‰åŸå§‹é“¾æ¥
- æ€»é•¿åº¦æ§åˆ¶åœ¨ 2500 å­—ä»¥å†…
- ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æ„åŒ–å…ƒç´ æå‡å¯è¯»æ€§
- **å¿…é¡»çªå‡ºæ˜¾ç¤º7å¤©å†…å³å°†åˆ°æ¥çš„é¢è¯•**

è¯·ç›´æ¥è¾“å‡ºåˆ†ææŠ¥å‘Šï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜ã€‚"""

    # ç»“æ„åŒ–ä¿¡æ¯æå– Prompt
    EXTRACT_INTERVIEW_INFO_PROMPT = """ä»ä»¥ä¸‹æ•™å¸ˆæ‹›è˜å…¬å‘Šä¸­æå–**ç»“æ„åŒ–é¢è¯•**ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š

å…¬å‘Šæ–‡æœ¬:
{text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆä»¥ JSON æ ¼å¼è¿”å›ï¼‰:
{{
    "region": "åœ°åŒºåç§°ï¼ˆçœ/å¸‚ï¼‰",
    "organization": "æ‹›è˜å•ä½åç§°",
    "announcement_title": "å…¬å‘Šæ ‡é¢˜",
    "announcement_url": "å…¬å‘Šé“¾æ¥",
    "recruitment_count": æ‹›è˜äººæ•°ï¼ˆæ•°å­—ï¼‰,
    "registration_period": {{
        "start": "æŠ¥åå¼€å§‹æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰",
        "end": "æŠ¥åæˆªæ­¢æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰"
    }},
    "written_exam_date": "ç¬”è¯•æ—¶é—´ï¼ˆYYYY-MM-DDï¼Œå¦‚æ— åˆ™nullï¼‰",
    "structured_interview": {{
        "has_interview": true/falseï¼ˆæ˜¯å¦åŒ…å«ç»“æ„åŒ–é¢è¯•ï¼‰,
        "interview_date": "é¢è¯•æ—¥æœŸï¼ˆYYYY-MM-DDï¼Œå¦‚æœªç¡®å®šåˆ™nullï¼‰",
        "interview_time": "é¢è¯•å…·ä½“æ—¶é—´ï¼ˆå¦‚æœ‰ï¼‰",
        "interview_location": "é¢è¯•åœ°ç‚¹",
        "interview_format": "é¢è¯•å½¢å¼ï¼ˆå¦‚ï¼šçº¯ç»“æ„åŒ–ã€ç»“æ„åŒ–+è¯•è®²ã€ç»“æ„åŒ–+è¯´è¯¾ï¼‰",
        "question_types": ["é¢˜å‹1", "é¢˜å‹2"],
        "interview_duration": "é¢è¯•æ—¶é•¿ï¼ˆå¦‚ï¼š15åˆ†é’Ÿï¼‰",
        "preparation_time": "å¤‡è€ƒæ—¶é—´ï¼ˆå¦‚ï¼š5åˆ†é’Ÿï¼‰"
    }},
    "special_requirements": "ç‰¹æ®Šè¦æ±‚æˆ–å¤‡æ³¨",
    "publish_date": "å…¬å‘Šå‘å¸ƒæ—¶é—´ï¼ˆYYYY-MM-DDï¼‰"
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœå…¬å‘Šä¸­æ²¡æœ‰æ˜ç¡®æåˆ°"ç»“æ„åŒ–é¢è¯•"ï¼Œåˆ™ has_interview è®¾ä¸º false
2. åªæå–æ˜ç¡®çš„ä¿¡æ¯ï¼Œä¸ç¡®å®šçš„å­—æ®µè®¾ä¸º null
3. é¢è¯•å½¢å¼éœ€è¦æ ¹æ®å…¬å‘Šæè¿°å‡†ç¡®åˆ¤æ–­ï¼ˆå¦‚"ç­”è¾©"ã€"é—®ç­”"é€šå¸¸æŒ‡ç»“æ„åŒ–é¢è¯•ï¼‰
4. æ—¶é—´æ ¼å¼ç»Ÿä¸€ä¸º YYYY-MM-DD
5. å¿…é¡»è¿”å›çº¯ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—è¯´æ˜
"""

    def __init__(self, api_key: str, base_url: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            api_key: Anthropic API å¯†é’¥
            base_url: API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
        """
        self.client = anthropic.Anthropic(api_key=api_key, base_url=base_url)
        self.ai_config = {
            "model": "glm-4-plus",
            "max_tokens": 8192,
            "temperature": 0.3
        }

    def generate_interview_digest(
        self,
        announcements: List[Dict],
        questions: List[Dict],
        today: str
    ) -> str:
        """
        ç”Ÿæˆç»“æ„åŒ–é¢è¯•è€ƒæƒ…ç®€æŠ¥

        Args:
            announcements: å…¬å‘Šåˆ—è¡¨
            questions: çœŸé¢˜åˆ—è¡¨
            today: ä»Šå¤©çš„æ—¥æœŸ

        Returns:
            ç”Ÿæˆçš„ç®€æŠ¥å†…å®¹
        """
        # ğŸ”’ æ•°æ®éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ•°æ®
        data_status = self._validate_data_before_analysis(announcements, questions)

        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨é™æ€æ¨¡æ¿
        if not data_status["has_data"]:
            print(f"\nâš ï¸  æ•°æ®çŠ¶æ€æ£€æŸ¥:")
            print(f"  - å…¬å‘Šæ•°é‡: {len(announcements)}")
            print(f"  - çœŸé¢˜æ•°é‡: {len(questions)}")
            print(f"  - æ•°æ®çŠ¶æ€: æ— æœ‰æ•ˆæ•°æ®")
            print(f"  - ä½¿ç”¨é™æ€ç®€æŠ¥æ¨¡æ¿")
            return self._generate_static_digest(
                announcements,
                questions,
                today,
                data_status["is_scraping_normal"]
            )

        # å‡†å¤‡å†…å®¹
        content = self._prepare_content(announcements, questions)

        print(f"\nğŸ¤– è°ƒç”¨ Claude API ç”Ÿæˆç®€æŠ¥...")
        print(f"  - å…¬å‘Šæ•°é‡: {len(announcements)}")
        print(f"  - çœŸé¢˜æ•°é‡: {len(questions)}")
        print(f"  - æ•°æ®çŠ¶æ€: æœ‰æœ‰æ•ˆæ•°æ®")

        try:
            # å‡†å¤‡ç©ºæ•°æ®æç¤º
            empty_data_notice = ""
            if len(announcements) == 0:
                empty_data_notice = "**é‡è¦æé†’**: ä»Šæ—¥æœªæ”¶é›†åˆ°æ–°çš„å…¬å‘Šä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜æ­¤æƒ…å†µï¼Œä¸è¦ç¼–é€ ä»»ä½•å†…å®¹ã€‚"

            # è°ƒç”¨ Claude API
            response = self.client.messages.create(
                model=self.ai_config["model"],
                max_tokens=self.ai_config["max_tokens"],
                temperature=self.ai_config["temperature"],
                messages=[{
                    "role": "user",
                    "content": self.STRUCTURED_INTERVIEW_ANALYSIS_PROMPT.format(
                        today=today,
                        content=content,
                        announcement_count=len(announcements),
                        question_count=len(questions),
                        data_status="æœ‰æ•°æ®" if len(announcements) > 0 else "æ— æ•°æ®",
                        empty_data_notice=empty_data_notice
                    )
                }]
            )

            digest = response.content[0].text
            print(f"âœ… ç®€æŠ¥ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(digest)} å­—ç¬¦")
            return digest

        except Exception as e:
            print(f"âŒ ç”Ÿæˆç®€æŠ¥å¤±è´¥: {e}")
            return self._generate_static_digest(
                announcements,
                questions,
                today,
                is_scraping_normal=False
            )

    def extract_interview_info(self, announcement_text: str, url: str) -> Dict:
        """
        ä»å…¬å‘Šä¸­æå–ç»“æ„åŒ–é¢è¯•ä¿¡æ¯

        Args:
            announcement_text: å…¬å‘Šæ–‡æœ¬
            url: å…¬å‘Š URL

        Returns:
            æå–çš„ç»“æ„åŒ–ä¿¡æ¯
        """
        try:
            prompt = self.EXTRACT_INTERVIEW_INFO_PROMPT.format(
                text=announcement_text[:5000]  # é™åˆ¶é•¿åº¦
            )

            response = self.client.messages.create(
                model=self.ai_config["model"],
                max_tokens=2048,
                temperature=0.2,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # è§£æ JSON ç»“æœ
            result_text = response.content[0].text
            result = json.loads(result_text)
            result['announcement_url'] = url

            return result

        except Exception as e:
            print(f"  âš ï¸  æå–ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "has_interview": False,
                "error": str(e)
            }

    def _prepare_content(self, announcements: List[Dict], questions: List[Dict]) -> str:
        """å‡†å¤‡å‘é€ç»™ AI çš„å†…å®¹"""
        content_parts = []

        # æ·»åŠ å…¬å‘Šä¿¡æ¯
        if announcements:
            content_parts.append("## æ‹›è˜å…¬å‘Šä¿¡æ¯\n")
            for i, ann in enumerate(announcements[:20], 1):  # é™åˆ¶æ•°é‡
                content_parts.append(f"{i}. **{ann.get('title', 'æœªçŸ¥æ ‡é¢˜')}**")
                content_parts.append(f"   - åœ°åŒº: {ann.get('region', 'æœªçŸ¥')}")
                content_parts.append(f"   - é“¾æ¥: {ann.get('url', 'æ— ')}")
                if 'found_at' in ann:
                    content_parts.append(f"   - å‘ç°æ—¶é—´: {ann['found_at']}")
                content_parts.append("")

        # æ·»åŠ çœŸé¢˜ä¿¡æ¯
        if questions:
            content_parts.append("\n## é¢è¯•çœŸé¢˜ä¿¡æ¯\n")
            for i, q in enumerate(questions[:10], 1):
                content_parts.append(f"{i}. {q.get('question', 'æœªçŸ¥é¢˜ç›®')}")

        return '\n'.join(content_parts)

    def _validate_data_before_analysis(
        self,
        announcements: List[Dict],
        questions: List[Dict]
    ) -> Dict:
        """
        éªŒè¯æ•°æ®æ˜¯å¦è¶³å¤Ÿè¿›è¡Œåˆ†æ

        Returns:
            åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸:
            - has_data: æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
            - is_scraping_normal: æŠ“å–æ˜¯å¦æ­£å¸¸å·¥ä½œ
        """
        # åˆ¤æ–­æ˜¯å¦æœ‰æ•°æ®
        has_announcements = len(announcements) > 0
        has_questions = len(questions) > 0

        # å¦‚æœæœ‰å…¬å‘Šæˆ–çœŸé¢˜ï¼Œè®¤ä¸ºæœ‰æ•°æ®
        has_data = has_announcements or has_questions

        # åˆ¤æ–­æŠ“å–æ˜¯å¦æ­£å¸¸ï¼ˆè¿™é‡Œç®€å•åˆ¤æ–­ï¼Œå®é™…å¯ä»¥æ›´å¤æ‚ï¼‰
        # å¦‚æœå®Œå…¨æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½æ˜¯æŠ“å–å¤±è´¥ï¼Œä¹Ÿå¯èƒ½æ˜¯çœŸçš„æ²¡æœ‰æ–°å…¬å‘Š
        is_scraping_normal = True  # é»˜è®¤è®¤ä¸ºæ­£å¸¸

        return {
            "has_data": has_data,
            "is_scraping_normal": is_scraping_normal
        }

    def _generate_static_digest(
        self,
        announcements: List[Dict],
        questions: List[Dict],
        today: str,
        is_scraping_normal: bool = True
    ) -> str:
        """
        ç”Ÿæˆé™æ€ç®€æŠ¥ï¼ˆç©ºæ•°æ®æ—¶ä½¿ç”¨ï¼‰

        Args:
            announcements: å…¬å‘Šåˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            questions: çœŸé¢˜åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            today: ä»Šå¤©çš„æ—¥æœŸ
            is_scraping_normal: æŠ“å–æ˜¯å¦æ­£å¸¸

        Returns:
            é™æ€ç®€æŠ¥å†…å®¹
        """
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        return self.STATIC_DIGEST_TEMPLATE.format(
            today=today,
            datetime=now,
            announcement_count=len(announcements),
            question_count=len(questions),
            is_normal="æ˜¯" if is_scraping_normal else "å¦ï¼ˆå¯èƒ½å­˜åœ¨é—®é¢˜ï¼‰"
        )

    def _generate_fallback_digest(self, announcements: List[Dict], today: str) -> str:
        """ç”Ÿæˆç®€åŒ–çš„å¤‡ç”¨ç®€æŠ¥ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰"""
        lines = [
            f"# æ•™å¸ˆè€ƒç¼–ç»“æ„åŒ–é¢è¯•è€ƒæƒ…ç®€æŠ¥ ({today})",
            "",
            "âš ï¸ **æ³¨æ„**: AI ç”Ÿæˆå¤±è´¥ï¼Œä»¥ä¸‹ä¸ºç®€åŒ–ç‰ˆæœ¬",
            "",
            "## ğŸ“Š ä»Šæ—¥æ•°æ®ç»Ÿè®¡",
            f"- æŠ“å–åˆ° {len(announcements)} æ¡ç›¸å…³å…¬å‘Š",
            "",
            "## ğŸ“‹ å…¬å‘Šåˆ—è¡¨",
            ""
        ]

        for i, ann in enumerate(announcements[:20], 1):
            lines.append(f"{i}. **{ann.get('title', 'æœªçŸ¥')}**")
            lines.append(f"   - åœ°åŒº: {ann.get('region', 'æœªçŸ¥')}")
            lines.append(f"   - é“¾æ¥: {ann.get('url', 'æ— ')}")
            lines.append("")

        return '\n'.join(lines)
